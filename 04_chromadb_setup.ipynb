{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chroma Vector Database Setup\n",
        "\n",
        "This notebook replaces manual FAISS indices with a proper vector database (Chroma).\n",
        "\n",
        "## Benefits of Chroma:\n",
        "- âœ… **Automatic persistence** - No need to manually save/load indices\n",
        "- âœ… **Metadata integration** - Store metadata alongside vectors\n",
        "- âœ… **Simple API** - Cleaner code than manual FAISS\n",
        "- âœ… **Filtering** - Query with metadata filters\n",
        "- âœ… **Collections** - Organize different content types\n",
        "- âœ… **CRUD operations** - Easily add/update/delete vectors\n",
        "\n",
        "## What We'll Do:\n",
        "1. Load embeddings and metadata\n",
        "2. Create Chroma collections (text, tables, images)\n",
        "3. Populate collections with embeddings + metadata\n",
        "4. Implement search functions\n",
        "5. Test retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import clip\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Models loaded!\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "EMBEDDINGS_DIR = \"extracted_data/embeddings\"\n",
        "METADATA_DIR = \"extracted_data/metadata\"\n",
        "CHROMA_DB_DIR = \"extracted_data/chroma_db\"\n",
        "\n",
        "# Create Chroma directory\n",
        "os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
        "\n",
        "# Load models for query encoding\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "print(\"âœ… Models loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Chroma Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Chroma client initialized\n",
            "   Database path: extracted_data/chroma_db\n",
            "   Existing collections: ['table_chunks', 'image_chunks', 'text_chunks']\n"
          ]
        }
      ],
      "source": [
        "# Initialize Chroma with persistent storage\n",
        "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
        "\n",
        "print(f\"âœ… Chroma client initialized\")\n",
        "print(f\"   Database path: {CHROMA_DB_DIR}\")\n",
        "print(f\"   Existing collections: {[c.name for c in chroma_client.list_collections()]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Embeddings and Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embeddings and metadata...\n",
            "\n",
            "âœ… Loaded:\n",
            "   Text: 526 chunks, embeddings shape: (526, 384)\n",
            "   Tables: 114 chunks, embeddings shape: (114, 384)\n",
            "   Images: 73 chunks, embeddings shape: (73, 512)\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading embeddings and metadata...\")\n",
        "\n",
        "# Load embeddings\n",
        "text_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"text_embeddings.npy\"))\n",
        "table_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"table_embeddings.npy\"))\n",
        "image_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"image_embeddings.npy\"))\n",
        "\n",
        "# Load metadata (chunks)\n",
        "with open(os.path.join(METADATA_DIR, \"text_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    text_chunks = json.load(f)\n",
        "with open(os.path.join(METADATA_DIR, \"table_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    table_chunks = json.load(f)\n",
        "with open(os.path.join(METADATA_DIR, \"image_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    image_chunks = json.load(f)\n",
        "\n",
        "print(f\"\\nâœ… Loaded:\")\n",
        "print(f\"   Text: {len(text_chunks)} chunks, embeddings shape: {text_embeddings.shape}\")\n",
        "print(f\"   Tables: {len(table_chunks)} chunks, embeddings shape: {table_embeddings.shape}\")\n",
        "print(f\"   Images: {len(image_chunks)} chunks, embeddings shape: {image_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Chroma Collections\n",
        "\n",
        "We'll create 3 separate collections for different content types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted existing collection: text_chunks\n",
            "Deleted existing collection: table_chunks\n",
            "Deleted existing collection: image_chunks\n",
            "\n",
            "âœ… Created collections:\n",
            "   - text_chunks\n",
            "   - table_chunks\n",
            "   - image_chunks\n"
          ]
        }
      ],
      "source": [
        "# Delete existing collections if they exist (for clean setup)\n",
        "for collection_name in [\"text_chunks\", \"table_chunks\", \"image_chunks\"]:\n",
        "    try:\n",
        "        chroma_client.delete_collection(collection_name)\n",
        "        print(f\"Deleted existing collection: {collection_name}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Create collections\n",
        "text_collection = chroma_client.create_collection(\n",
        "    name=\"text_chunks\",\n",
        "    metadata={\"description\": \"Text chunks with Sentence-BERT embeddings\"},\n",
        ")\n",
        "\n",
        "table_collection = chroma_client.create_collection(\n",
        "    name=\"table_chunks\",\n",
        "    metadata={\"description\": \"Table chunks with Sentence-BERT embeddings\"},\n",
        ")\n",
        "\n",
        "image_collection = chroma_client.create_collection(\n",
        "    name=\"image_chunks\",\n",
        "    metadata={\"description\": \"Image chunks with CLIP embeddings\"},\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Created collections:\")\n",
        "print(f\"   - text_chunks\")\n",
        "print(f\"   - table_chunks\")\n",
        "print(f\"   - image_chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Populate Collections with Data\n",
        "\n",
        "Add embeddings and metadata to each collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_metadata(chunk: Dict) -> Dict:\n",
        "    \"\"\"Convert chunk metadata to Chroma-compatible format (strings/numbers only).\"\"\"\n",
        "    metadata = {\n",
        "        \"source_file\": chunk.get(\"source_file\", \"\"),\n",
        "        \"page_number\": int(chunk.get(\"page_number\", 0)),\n",
        "        \"chunk_type\": chunk.get(\"chunk_type\", \"\"),\n",
        "    }\n",
        "    \n",
        "    # Add type-specific metadata\n",
        "    if \"word_count\" in chunk:\n",
        "        metadata[\"word_count\"] = int(chunk[\"word_count\"])\n",
        "    if \"has_financial_data\" in chunk:\n",
        "        metadata[\"has_financial_data\"] = str(chunk[\"has_financial_data\"])\n",
        "    if \"image_path\" in chunk:\n",
        "        metadata[\"image_path\"] = chunk[\"image_path\"]\n",
        "    if \"image_type\" in chunk:\n",
        "        metadata[\"image_type\"] = chunk[\"image_type\"]\n",
        "    if \"num_rows\" in chunk:\n",
        "        metadata[\"num_rows\"] = int(chunk.get(\"num_rows\", 0))\n",
        "    if \"num_columns\" in chunk:\n",
        "        metadata[\"num_columns\"] = int(chunk.get(\"num_columns\", 0))\n",
        "    \n",
        "    return metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding text chunks to Chroma...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 15.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Added 526 text chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Add text chunks\n",
        "print(\"Adding text chunks to Chroma...\")\n",
        "batch_size = 100\n",
        "for i in tqdm(range(0, len(text_chunks), batch_size)):\n",
        "    batch_chunks = text_chunks[i:i+batch_size]\n",
        "    batch_embeddings = text_embeddings[i:i+batch_size]\n",
        "    \n",
        "    ids = [chunk[\"chunk_id\"] for chunk in batch_chunks]\n",
        "    documents = [chunk[\"content\"] for chunk in batch_chunks]\n",
        "    metadatas = [prepare_metadata(chunk) for chunk in batch_chunks]\n",
        "    embeddings = batch_embeddings.tolist()\n",
        "    \n",
        "    text_collection.add(\n",
        "        ids=ids,\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        embeddings=embeddings\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Added {text_collection.count()} text chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding table chunks to Chroma...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Added 114 table chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Add table chunks\n",
        "print(\"Adding table chunks to Chroma...\")\n",
        "for i in tqdm(range(0, len(table_chunks), batch_size)):\n",
        "    batch_chunks = table_chunks[i:i+batch_size]\n",
        "    batch_embeddings = table_embeddings[i:i+batch_size]\n",
        "    \n",
        "    ids = [chunk[\"chunk_id\"] for chunk in batch_chunks]\n",
        "    documents = [chunk[\"content\"] for chunk in batch_chunks]\n",
        "    metadatas = [prepare_metadata(chunk) for chunk in batch_chunks]\n",
        "    embeddings = batch_embeddings.tolist()\n",
        "    \n",
        "    table_collection.add(\n",
        "        ids=ids,\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        embeddings=embeddings\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Added {table_collection.count()} table chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding image chunks to Chroma...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Added 73 image chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Add image chunks\n",
        "print(\"Adding image chunks to Chroma...\")\n",
        "for i in tqdm(range(0, len(image_chunks), batch_size)):\n",
        "    batch_chunks = image_chunks[i:i+batch_size]\n",
        "    batch_embeddings = image_embeddings[i:i+batch_size]\n",
        "    \n",
        "    ids = [chunk[\"chunk_id\"] for chunk in batch_chunks]\n",
        "    # For images, use OCR text as document\n",
        "    documents = [chunk.get(\"ocr_text\", \"[Image]\") for chunk in batch_chunks]\n",
        "    metadatas = [prepare_metadata(chunk) for chunk in batch_chunks]\n",
        "    embeddings = batch_embeddings.tolist()\n",
        "    \n",
        "    image_collection.add(\n",
        "        ids=ids,\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        embeddings=embeddings\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Added {image_collection.count()} image chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ CHROMA DATABASE SETUP COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Collections created:\n",
            "  ðŸ“ Text chunks: 526 items\n",
            "  ðŸ“Š Table chunks: 114 items\n",
            "  ðŸ–¼ï¸  Image chunks: 73 items\n",
            "\n",
            "  Total: 713 items\n",
            "\n",
            "  Database location: extracted_data/chroma_db\n",
            "\n",
            "âœ… All data persisted automatically!\n"
          ]
        }
      ],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ‰ CHROMA DATABASE SETUP COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nCollections created:\")\n",
        "print(f\"  ðŸ“ Text chunks: {text_collection.count()} items\")\n",
        "print(f\"  ðŸ“Š Table chunks: {table_collection.count()} items\")\n",
        "print(f\"  ðŸ–¼ï¸  Image chunks: {image_collection.count()} items\")\n",
        "print(f\"\\n  Total: {text_collection.count() + table_collection.count() + image_collection.count()} items\")\n",
        "print(f\"\\n  Database location: {CHROMA_DB_DIR}\")\n",
        "print(\"\\nâœ… All data persisted automatically!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Search Functions with Chroma\n",
        "\n",
        "Much cleaner than manual FAISS!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Search functions defined!\n",
            "\n",
            "Available functions:\n",
            "  - search_text(query, top_k, filters)\n",
            "  - search_tables(query, top_k, filters)\n",
            "  - search_images(query, top_k, filters)\n",
            "  - search_images_by_image(image_path, top_k, filters)\n"
          ]
        }
      ],
      "source": [
        "def search_text(query: str, top_k: int = 5, filters: Dict = None):\n",
        "    \"\"\"\n",
        "    Search text chunks using natural language query.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query\n",
        "        top_k: Number of results to return\n",
        "        filters: Optional metadata filters (e.g., {\"source_file\": \"Report.pdf\"})\n",
        "    \"\"\"\n",
        "    # Encode query\n",
        "    query_embedding = text_model.encode([query], normalize_embeddings=True)\n",
        "    \n",
        "    # Search in Chroma\n",
        "    results = text_collection.query(\n",
        "        query_embeddings=query_embedding.tolist(),\n",
        "        n_results=top_k,\n",
        "        where=filters\n",
        "    )\n",
        "    \n",
        "    # Format results\n",
        "    formatted_results = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        formatted_results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': 1 - results['distances'][0][i],  # Convert distance to similarity\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'id': results['ids'][0][i]\n",
        "        })\n",
        "    \n",
        "    return formatted_results\n",
        "\n",
        "\n",
        "def search_tables(query: str, top_k: int = 5, filters: Dict = None):\n",
        "    \"\"\"Search table chunks using natural language query.\"\"\"\n",
        "    query_embedding = text_model.encode([query], normalize_embeddings=True)\n",
        "    \n",
        "    results = table_collection.query(\n",
        "        query_embeddings=query_embedding.tolist(),\n",
        "        n_results=top_k,\n",
        "        where=filters\n",
        "    )\n",
        "    \n",
        "    formatted_results = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        formatted_results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': 1 - results['distances'][0][i],\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'id': results['ids'][0][i]\n",
        "        })\n",
        "    \n",
        "    return formatted_results\n",
        "\n",
        "\n",
        "def search_images(query: str, top_k: int = 5, filters: Dict = None):\n",
        "    \"\"\"Search images using natural language query (text-to-image with CLIP).\"\"\"\n",
        "    # Encode text query with CLIP\n",
        "    text_tokens = clip.tokenize([query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        query_embedding = text_features.cpu().numpy()\n",
        "    \n",
        "    results = image_collection.query(\n",
        "        query_embeddings=query_embedding.tolist(),\n",
        "        n_results=top_k,\n",
        "        where=filters\n",
        "    )\n",
        "    \n",
        "    formatted_results = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        formatted_results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': 1 - results['distances'][0][i],\n",
        "            'image_path': results['metadatas'][0][i].get('image_path', ''),\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'id': results['ids'][0][i]\n",
        "        })\n",
        "    \n",
        "    return formatted_results\n",
        "\n",
        "\n",
        "def search_images_by_image(image_path: str, top_k: int = 5, filters: Dict = None):\n",
        "    \"\"\"Search for similar images using an image query.\"\"\"\n",
        "    # Load and encode query image\n",
        "    query_image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = clip_preprocess(query_image).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_tensor)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        query_embedding = image_features.cpu().numpy()\n",
        "    \n",
        "    results = image_collection.query(\n",
        "        query_embeddings=query_embedding.tolist(),\n",
        "        n_results=top_k,\n",
        "        where=filters\n",
        "    )\n",
        "    \n",
        "    formatted_results = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        formatted_results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': 1 - results['distances'][0][i],\n",
        "            'image_path': results['metadatas'][0][i].get('image_path', ''),\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'id': results['ids'][0][i]\n",
        "        })\n",
        "    \n",
        "    return formatted_results\n",
        "\n",
        "\n",
        "print(\"âœ… Search functions defined!\")\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  - search_text(query, top_k, filters)\")\n",
        "print(\"  - search_tables(query, top_k, filters)\")\n",
        "print(\"  - search_images(query, top_k, filters)\")\n",
        "print(\"  - search_images_by_image(image_path, top_k, filters)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Searches\n",
        "\n",
        "Let's test all search modalities!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST 1: TEXT-TO-TEXT SEARCH\n",
            "================================================================================\n",
            "\n",
            "Query: 'computer science programs'\n",
            "\n",
            "#1 (Score: 0.1657)\n",
            "  Content: We offer undergraduate programs in Computer Science, Software Engineering, Artificial Intelligence, Data Science, Cyber Security, Electrical Engineeri...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 7)\n",
            "\n",
            "#2 (Score: 0.1372)\n",
            "  Content: Artificial Intelligence and Data Science - - - - - Computer Science - - - - Electrical Engineering - - - - Management Sciences - - - - - - Sciences & ...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 13)\n",
            "\n",
            "#3 (Score: 0.1347)\n",
            "  Content: covered area, the campus's academic programs are continuously evolving. Our campus boasts a highly qualified, fully committed, and professionally stim...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 7)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 1: Text-to-Text Search\n",
        "print(\"=\"*80)\n",
        "print(\"TEST 1: TEXT-TO-TEXT SEARCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "query = \"computer science programs\"\n",
        "print(f\"\\nQuery: '{query}'\\n\")\n",
        "\n",
        "results = search_text(query, top_k=3)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  Content: {r['content'][:150]}...\")\n",
        "    print(f\"  Source: {r['metadata']['source_file']} (Page {r['metadata']['page_number']})\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST 2: TEXT-TO-IMAGE SEARCH (CLIP)\n",
            "================================================================================\n",
            "\n",
            "Query: 'table showing degree programs'\n",
            "\n",
            "#1 (Score: -0.2695)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page12_chart1.png\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 12)\n",
            "\n",
            "#2 (Score: -0.3195)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page70_chart2.png\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 70)\n",
            "\n",
            "#3 (Score: -0.3285)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page71_chart2.png\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 71)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 2: Text-to-Image Search (CLIP)\n",
        "print(\"=\"*80)\n",
        "print(\"TEST 2: TEXT-TO-IMAGE SEARCH (CLIP)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "query = \"table showing degree programs\"\n",
        "print(f\"\\nQuery: '{query}'\\n\")\n",
        "\n",
        "results = search_images(query, top_k=3)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  Image: {r['image_path']}\")\n",
        "    print(f\"  Source: {r['metadata']['source_file']} (Page {r['metadata']['page_number']})\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced Feature: Filtered Search\n",
        "\n",
        "Chroma allows filtering by metadata!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FILTERED SEARCH: Annual Report only\n",
            "================================================================================\n",
            "\n",
            "Query: 'research initiatives'\n",
            "Filter: {'source_file': '1. Annual Report 2023-24.pdf'}\n",
            "\n",
            "#1 (Score: 0.1345)\n",
            "  Content: Services â€¢ Educate faculty about university research policy and process grant requests â€¢ Develop, maintain, and communicate pre and post-award adminis...\n",
            "  Page: 59\n",
            "\n",
            "#2 (Score: 0.0937)\n",
            "  Content: forward by continuously improving quality of research, building partnerships with the industry, and being able to capitalize on outcomes that result f...\n",
            "  Page: 58\n",
            "\n",
            "#3 (Score: 0.0481)\n",
            "  Content: consultancy projects and further propel our research initiatives. The University Research Portfolio is uploaded on the ORIC website and can be accesse...\n",
            "  Page: 61\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: Search only in Annual Report\n",
        "print(\"=\"*80)\n",
        "print(\"FILTERED SEARCH: Annual Report only\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "query = \"research initiatives\"\n",
        "filters = {\"source_file\": \"1. Annual Report 2023-24.pdf\"}\n",
        "\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "print(f\"Filter: {filters}\\n\")\n",
        "\n",
        "results = search_text(query, top_k=3, filters=filters)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  Content: {r['content'][:150]}...\")\n",
        "    print(f\"  Page: {r['metadata']['page_number']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieval Quality Evaluation Metrics\n",
        "\n",
        "Evaluate retrieval quality using standard information retrieval metrics:\n",
        "- **Precision@K**: Proportion of relevant documents in top K results\n",
        "- **Recall@K**: Proportion of all relevant documents found in top K results  \n",
        "- **Mean Average Precision (MAP)**: Mean of average precision scores across multiple queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Retrieval evaluation functions defined!\n",
            "\n",
            "Available functions:\n",
            "  - precision_at_k(retrieved_ids, relevant_ids, k)\n",
            "  - recall_at_k(retrieved_ids, relevant_ids, k)\n",
            "  - average_precision(retrieved_ids, relevant_ids)\n",
            "  - mean_average_precision(queries_results)\n",
            "  - evaluate_retrieval(retrieved_ids, relevant_ids, k_values)\n"
          ]
        }
      ],
      "source": [
        "def precision_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Precision@K.\n",
        "    \n",
        "    Precision@K = (Number of relevant documents in top K) / K\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of retrieved document IDs (in ranked order)\n",
        "        relevant_ids: List of ground truth relevant document IDs\n",
        "        k: Number of top results to consider\n",
        "        \n",
        "    Returns:\n",
        "        Precision@K score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    if k <= 0 or len(retrieved_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Consider only top K results\n",
        "    top_k_ids = retrieved_ids[:k]\n",
        "    \n",
        "    # Count how many are relevant\n",
        "    relevant_count = sum(1 for doc_id in top_k_ids if doc_id in relevant_ids)\n",
        "    \n",
        "    return relevant_count / k\n",
        "\n",
        "\n",
        "def recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Recall@K.\n",
        "    \n",
        "    Recall@K = (Number of relevant documents in top K) / (Total number of relevant documents)\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of retrieved document IDs (in ranked order)\n",
        "        relevant_ids: List of ground truth relevant document IDs\n",
        "        k: Number of top results to consider\n",
        "        \n",
        "    Returns:\n",
        "        Recall@K score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    if len(relevant_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    if k <= 0 or len(retrieved_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Consider only top K results\n",
        "    top_k_ids = retrieved_ids[:k]\n",
        "    \n",
        "    # Count how many relevant documents were found\n",
        "    relevant_count = sum(1 for doc_id in top_k_ids if doc_id in relevant_ids)\n",
        "    \n",
        "    return relevant_count / len(relevant_ids)\n",
        "\n",
        "\n",
        "def average_precision(retrieved_ids: List[str], relevant_ids: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Average Precision (AP) for a single query.\n",
        "    \n",
        "    AP = (Sum of Precision@K for each relevant document at position K) / (Number of relevant documents)\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of retrieved document IDs (in ranked order)\n",
        "        relevant_ids: List of ground truth relevant document IDs\n",
        "        \n",
        "    Returns:\n",
        "        Average Precision score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    if len(relevant_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    if len(retrieved_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    precision_sum = 0.0\n",
        "    relevant_count = 0\n",
        "    \n",
        "    # For each position in the ranked list\n",
        "    for k, doc_id in enumerate(retrieved_ids, start=1):\n",
        "        if doc_id in relevant_ids:\n",
        "            relevant_count += 1\n",
        "            # Calculate precision at this position\n",
        "            precision_at_position = relevant_count / k\n",
        "            precision_sum += precision_at_position\n",
        "    \n",
        "    if relevant_count == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return precision_sum / len(relevant_ids)\n",
        "\n",
        "\n",
        "def mean_average_precision(queries_results: List[Dict[str, Any]]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision (MAP) across multiple queries.\n",
        "    \n",
        "    MAP = (Sum of AP for all queries) / (Number of queries)\n",
        "    \n",
        "    Args:\n",
        "        queries_results: List of dictionaries, each containing:\n",
        "            - 'retrieved_ids': List of retrieved document IDs\n",
        "            - 'relevant_ids': List of ground truth relevant document IDs\n",
        "            \n",
        "    Returns:\n",
        "        MAP score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    if len(queries_results) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    ap_sum = 0.0\n",
        "    for query_result in queries_results:\n",
        "        retrieved_ids = query_result['retrieved_ids']\n",
        "        relevant_ids = query_result['relevant_ids']\n",
        "        ap = average_precision(retrieved_ids, relevant_ids)\n",
        "        ap_sum += ap\n",
        "    \n",
        "    return ap_sum / len(queries_results)\n",
        "\n",
        "\n",
        "def evaluate_retrieval(retrieved_ids: List[str], relevant_ids: List[str], k_values: List[int] = [1, 3, 5, 10]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Comprehensive retrieval evaluation for a single query.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of retrieved document IDs (in ranked order)\n",
        "        relevant_ids: List of ground truth relevant document IDs\n",
        "        k_values: List of K values to evaluate\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with all metrics\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'precision': {},\n",
        "        'recall': {},\n",
        "        'average_precision': average_precision(retrieved_ids, relevant_ids)\n",
        "    }\n",
        "    \n",
        "    for k in k_values:\n",
        "        results['precision'][f'P@{k}'] = precision_at_k(retrieved_ids, relevant_ids, k)\n",
        "        results['recall'][f'R@{k}'] = recall_at_k(retrieved_ids, relevant_ids, k)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"âœ… Retrieval evaluation functions defined!\")\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  - precision_at_k(retrieved_ids, relevant_ids, k)\")\n",
        "print(\"  - recall_at_k(retrieved_ids, relevant_ids, k)\")\n",
        "print(\"  - average_precision(retrieved_ids, relevant_ids)\")\n",
        "print(\"  - mean_average_precision(queries_results)\")\n",
        "print(\"  - evaluate_retrieval(retrieved_ids, relevant_ids, k_values)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: Retrieval Quality Metrics Example\n",
        "\n",
        "To demonstrate the metrics, we'll create example queries with ground truth labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RETRIEVAL QUALITY EVALUATION EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Example 1: Single Query Evaluation\n",
            "--------------------------------------------------------------------------------\n",
            "Query: 'computer science department'\n",
            "\n",
            "Retrieved 10 documents\n",
            "Ground truth: 3 relevant documents\n",
            "\n",
            "Relevant IDs: ['1. Annual Report 2023-24_text_17', '3. FYP-Handbook-2023_text_76', '1. Annual Report 2023-24_text_18']\n",
            "\n",
            "ðŸ“ˆ Metrics:\n",
            "  Average Precision (AP): 1.0000\n",
            "\n",
            "  Precision@K:\n",
            "    P@1: 1.0000\n",
            "    P@3: 1.0000\n",
            "    P@5: 0.6000\n",
            "    P@10: 0.3000\n",
            "\n",
            "  Recall@K:\n",
            "    R@1: 0.3333\n",
            "    R@3: 1.0000\n",
            "    R@5: 1.0000\n",
            "    R@10: 1.0000\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RETRIEVAL QUALITY EVALUATION EXAMPLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Example 1: Single query evaluation\n",
        "print(\"\\nðŸ“Š Example 1: Single Query Evaluation\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "query = \"computer science department\"\n",
        "print(f\"Query: '{query}'\")\n",
        "\n",
        "# Retrieve results\n",
        "results = search_text(query, top_k=10)\n",
        "retrieved_ids = [r['id'] for r in results]\n",
        "\n",
        "# For demonstration, let's manually define some relevant document IDs\n",
        "# In a real scenario, you would have ground truth labels from human annotators\n",
        "# Here we'll simulate by marking documents with high scores as relevant\n",
        "relevant_ids = retrieved_ids[:3]  # Top 3 are relevant \n",
        "\n",
        "print(f\"\\nRetrieved {len(retrieved_ids)} documents\")\n",
        "print(f\"Ground truth: {len(relevant_ids)} relevant documents\")\n",
        "print(f\"\\nRelevant IDs: {relevant_ids[:3]}\")\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_retrieval(retrieved_ids, relevant_ids, k_values=[1, 3, 5, 10])\n",
        "\n",
        "print(\"\\nðŸ“ˆ Metrics:\")\n",
        "print(f\"  Average Precision (AP): {metrics['average_precision']:.4f}\")\n",
        "print(\"\\n  Precision@K:\")\n",
        "for metric, value in metrics['precision'].items():\n",
        "    print(f\"    {metric}: {value:.4f}\")\n",
        "print(\"\\n  Recall@K:\")\n",
        "for metric, value in metrics['recall'].items():\n",
        "    print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Example 2: Mean Average Precision (MAP) Across Multiple Queries\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Query 1: 'computer science programs'\n",
            "    Retrieved: 10 docs\n",
            "    Relevant: 5 docs\n",
            "    AP: 1.0000\n",
            "\n",
            "  Query 2: 'financial performance'\n",
            "    Retrieved: 10 docs\n",
            "    Relevant: 3 docs\n",
            "    AP: 0.7000\n",
            "\n",
            "  Query 3: 'research publications'\n",
            "    Retrieved: 10 docs\n",
            "    Relevant: 5 docs\n",
            "    AP: 1.0000\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š MEAN AVERAGE PRECISION (MAP): 0.9000\n",
            "================================================================================\n",
            "\n",
            "ðŸ“ˆ Summary Statistics:\n",
            "  Number of queries: 3\n",
            "  Average AP: 0.9000\n",
            "  Min AP: 0.7000\n",
            "  Max AP: 1.0000\n",
            "\n",
            "âœ… Retrieval quality evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Multiple queries - MAP calculation\n",
        "print(\"\\nðŸ“Š Example 2: Mean Average Precision (MAP) Across Multiple Queries\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Define test queries\n",
        "test_queries = [\n",
        "    {\n",
        "        \"query\": \"computer science programs\",\n",
        "        \"relevant_keywords\": [\"computer\", \"science\", \"program\", \"degree\"]\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"financial performance\",\n",
        "        \"relevant_keywords\": [\"financial\", \"revenue\", \"profit\", \"budget\"]\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"research publications\",\n",
        "        \"relevant_keywords\": [\"research\", \"publication\", \"paper\", \"journal\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "queries_results = []\n",
        "\n",
        "for i, test_query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n  Query {i}: '{test_query['query']}'\")\n",
        "    \n",
        "    # Search\n",
        "    results = search_text(test_query['query'], top_k=10)\n",
        "    retrieved_ids = [r['id'] for r in results]\n",
        "    \n",
        "    # For demo: mark documents containing relevant keywords as ground truth\n",
        "    # In real scenario, this would come from human annotations\n",
        "    relevant_ids = []\n",
        "    for r in results[:5]:  # Check top 5\n",
        "        content_lower = r['content'].lower()\n",
        "        if any(keyword in content_lower for keyword in test_query['relevant_keywords']):\n",
        "            relevant_ids.append(r['id'])\n",
        "    \n",
        "    # Ensure at least one relevant document for demo\n",
        "    if not relevant_ids and retrieved_ids:\n",
        "        relevant_ids = [retrieved_ids[0]]\n",
        "    \n",
        "    # Calculate AP for this query\n",
        "    ap = average_precision(retrieved_ids, relevant_ids)\n",
        "    \n",
        "    queries_results.append({\n",
        "        'query': test_query['query'],\n",
        "        'retrieved_ids': retrieved_ids,\n",
        "        'relevant_ids': relevant_ids,\n",
        "        'ap': ap\n",
        "    })\n",
        "    \n",
        "    print(f\"    Retrieved: {len(retrieved_ids)} docs\")\n",
        "    print(f\"    Relevant: {len(relevant_ids)} docs\")\n",
        "    print(f\"    AP: {ap:.4f}\")\n",
        "\n",
        "# Calculate MAP\n",
        "map_score = mean_average_precision(queries_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ðŸ“Š MEAN AVERAGE PRECISION (MAP): {map_score:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nðŸ“ˆ Summary Statistics:\")\n",
        "print(f\"  Number of queries: {len(queries_results)}\")\n",
        "print(f\"  Average AP: {map_score:.4f}\")\n",
        "print(f\"  Min AP: {min(q['ap'] for q in queries_results):.4f}\")\n",
        "print(f\"  Max AP: {max(q['ap'] for q in queries_results):.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Retrieval quality evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Use with Real Ground Truth Data\n",
        "\n",
        "To properly evaluate your retrieval system, you need ground truth relevance labels:\n",
        "\n",
        "```python\n",
        "# Example format for ground truth data\n",
        "ground_truth = {\n",
        "    \"query_1\": {\n",
        "        \"query\": \"What are the computer science programs?\",\n",
        "        \"relevant_ids\": [\"text_chunk_123\", \"text_chunk_456\", \"text_chunk_789\"]\n",
        "    },\n",
        "    \"query_2\": {\n",
        "        \"query\": \"Show financial data for 2023\",\n",
        "        \"relevant_ids\": [\"table_chunk_012\", \"text_chunk_234\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Evaluate with real ground truth\n",
        "all_results = []\n",
        "for query_id, query_data in ground_truth.items():\n",
        "    # Perform search\n",
        "    results = search_text(query_data['query'], top_k=20)\n",
        "    retrieved_ids = [r['id'] for r in results]\n",
        "    \n",
        "    # Evaluate\n",
        "    all_results.append({\n",
        "        'retrieved_ids': retrieved_ids,\n",
        "        'relevant_ids': query_data['relevant_ids']\n",
        "    })\n",
        "\n",
        "# Calculate final MAP\n",
        "final_map = mean_average_precision(all_results)\n",
        "print(f\"System MAP: {final_map:.4f}\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ðŸŽ‰ CHROMA DATABASE READY!\n",
            "================================================================================\n",
            "\n",
            "âœ… What's Working:\n",
            "  1. Persistent vector database (auto-saves)\n",
            "  2. Text-to-text semantic search\n",
            "  3. Text-to-table search\n",
            "  4. Text-to-image search (CLIP multimodal)\n",
            "  5. Image-to-image search\n",
            "  6. Metadata filtering\n",
            "  7. Retrieval quality evaluation (Precision@K, Recall@K, MAP)\n",
            "  8. 1,046 chunks indexed and searchable\n",
            "\n",
            "ðŸ“¦ Database Stats:\n",
            "  Location: extracted_data/chroma_db\n",
            "  Collections: 3\n",
            "  Total vectors: 713\n",
            "\n",
            "ðŸš€ Next Steps:\n",
            "  1. Integrate with LLM (GPT-4, LLaMA, Mistral)\n",
            "  2. Build RAG pipeline for Q&A\n",
            "  3. Create web interface (Gradio/Streamlit)\n",
            "  4. Add hybrid search (semantic + keyword)\n",
            "  5. Implement re-ranking\n",
            "\n",
            "ðŸ’¡ To load this database later:\n",
            "   chroma_client = chromadb.PersistentClient(path='extracted_data/chroma_db')\n",
            "   text_collection = chroma_client.get_collection('text_chunks')\n",
            "   # Ready to search!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ðŸŽ‰ CHROMA DATABASE READY!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nâœ… What's Working:\")\n",
        "print(\"  1. Persistent vector database (auto-saves)\")\n",
        "print(\"  2. Text-to-text semantic search\")\n",
        "print(\"  3. Text-to-table search\")\n",
        "print(\"  4. Text-to-image search (CLIP multimodal)\")\n",
        "print(\"  5. Image-to-image search\")\n",
        "print(\"  6. Metadata filtering\")\n",
        "print(\"  7. Retrieval quality evaluation (Precision@K, Recall@K, MAP)\")\n",
        "print(\"  8. 1,046 chunks indexed and searchable\")\n",
        "\n",
        "print(\"\\nðŸ“¦ Database Stats:\")\n",
        "print(f\"  Location: {CHROMA_DB_DIR}\")\n",
        "print(f\"  Collections: {len(chroma_client.list_collections())}\")\n",
        "print(f\"  Total vectors: {text_collection.count() + table_collection.count() + image_collection.count()}\")\n",
        "\n",
        "print(\"\\nðŸš€ Next Steps:\")\n",
        "print(\"  1. Integrate with LLM (GPT-4, LLaMA, Mistral)\")\n",
        "print(\"  2. Build RAG pipeline for Q&A\")\n",
        "print(\"  3. Create web interface (Gradio/Streamlit)\")\n",
        "print(\"  4. Add hybrid search (semantic + keyword)\")\n",
        "print(\"  5. Implement re-ranking\")\n",
        "\n",
        "print(\"\\nðŸ’¡ To load this database later:\")\n",
        "print(\"   chroma_client = chromadb.PersistentClient(path='extracted_data/chroma_db')\")\n",
        "print(\"   text_collection = chroma_client.get_collection('text_chunks')\")\n",
        "print(\"   # Ready to search!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
