{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Search System with FAISS\n",
        "\n",
        "This notebook implements a powerful vector search system using FAISS (Facebook AI Similarity Search).\n",
        "\n",
        "## Features\n",
        "- Load embeddings from previous step\n",
        "- Create FAISS indices for fast similarity search\n",
        "- Text-to-text search (find similar text chunks)\n",
        "- Text-to-table search (find relevant tables)\n",
        "- Text-to-image search (CLIP multimodal search)\n",
        "- Unified search interface\n",
        "- Result ranking and filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install FAISS (uncomment if needed)\n",
        "# %pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Union\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import clip\n",
        "import faiss\n",
        "from PIL import Image\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models loaded!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "EMBEDDINGS_DIR = \"extracted_data/embeddings\"\n",
        "METADATA_DIR = \"extracted_data/metadata\"\n",
        "INDICES_DIR = \"extracted_data/indices\"\n",
        "os.makedirs(INDICES_DIR, exist_ok=True)\n",
        "\n",
        "# Load models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(\"Models loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embeddings...\n",
            "Loaded: 859 text, 114 tables, 73 images\n"
          ]
        }
      ],
      "source": [
        "# Load embeddings and chunks\n",
        "print(\"Loading embeddings...\")\n",
        "text_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"text_embeddings.npy\"))\n",
        "table_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"table_embeddings.npy\"))\n",
        "image_embeddings = np.load(os.path.join(EMBEDDINGS_DIR, \"image_embeddings.npy\"))\n",
        "\n",
        "with open(os.path.join(METADATA_DIR, \"text_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    text_chunks = json.load(f)\n",
        "with open(os.path.join(METADATA_DIR, \"table_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    table_chunks = json.load(f)\n",
        "with open(os.path.join(METADATA_DIR, \"image_chunks.json\"), 'r', encoding='utf-8') as f:\n",
        "    image_chunks = json.load(f)\n",
        "\n",
        "print(f\"Loaded: {len(text_chunks)} text, {len(table_chunks)} tables, {len(image_chunks)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create FAISS Indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating FAISS indices...\n",
            "Text index: 859 vectors\n",
            "Table index: 114 vectors\n",
            "Image index: 73 vectors\n",
            "\n",
            "Indices saved to: extracted_data/indices\n"
          ]
        }
      ],
      "source": [
        "# Create FAISS indices for fast similarity search\n",
        "print(\"Creating FAISS indices...\")\n",
        "\n",
        "# Text index\n",
        "text_index = faiss.IndexFlatIP(text_embeddings.shape[1])  # Inner product for normalized vectors\n",
        "text_index.add(text_embeddings)\n",
        "faiss.write_index(text_index, os.path.join(INDICES_DIR, \"text_index.faiss\"))\n",
        "print(f\"Text index: {text_index.ntotal} vectors\")\n",
        "\n",
        "# Table index\n",
        "table_index = faiss.IndexFlatIP(table_embeddings.shape[1])\n",
        "table_index.add(table_embeddings)\n",
        "faiss.write_index(table_index, os.path.join(INDICES_DIR, \"table_index.faiss\"))\n",
        "print(f\"Table index: {table_index.ntotal} vectors\")\n",
        "\n",
        "# Image index\n",
        "image_index = faiss.IndexFlatIP(image_embeddings.shape[1])\n",
        "image_index.add(image_embeddings)\n",
        "faiss.write_index(image_index, os.path.join(INDICES_DIR, \"image_index.faiss\"))\n",
        "print(f\"Image index: {image_index.ntotal} vectors\")\n",
        "\n",
        "print(f\"\\nIndices saved to: {INDICES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Search Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search functions defined!\n"
          ]
        }
      ],
      "source": [
        "def search_text(query: str, top_k: int = 5):\n",
        "    \"\"\"Search for similar text chunks.\"\"\"\n",
        "    query_embedding = text_model.encode([query], normalize_embeddings=True)\n",
        "    distances, indices = text_index.search(query_embedding, top_k)\n",
        "    \n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if idx == -1:\n",
        "            break\n",
        "        chunk = text_chunks[idx]\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': float(distance),\n",
        "            'content': chunk['content'],\n",
        "            'source': chunk['source_file'],\n",
        "            'page': chunk['page_number']\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def search_tables(query: str, top_k: int = 5):\n",
        "    \"\"\"Search for similar table chunks.\"\"\"\n",
        "    query_embedding = text_model.encode([query], normalize_embeddings=True)\n",
        "    distances, indices = table_index.search(query_embedding, top_k)\n",
        "    \n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if idx == -1:\n",
        "            break\n",
        "        chunk = table_chunks[idx]\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': float(distance),\n",
        "            'content': chunk['content'],\n",
        "            'source': chunk['source_file'],\n",
        "            'page': chunk['page_number']\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def search_images(query: str, top_k: int = 5):\n",
        "    \"\"\"Text-to-image search using CLIP.\"\"\"\n",
        "    text_tokens = clip.tokenize([query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        query_embedding = text_features.cpu().numpy()\n",
        "    \n",
        "    distances, indices = image_index.search(query_embedding, top_k)\n",
        "    \n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if idx == -1:\n",
        "            break\n",
        "        chunk = image_chunks[idx]\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'score': float(distance),\n",
        "            'image_path': chunk['image_path'],\n",
        "            'source': chunk['source_file'],\n",
        "            'page': chunk['page_number'],\n",
        "            'type': chunk.get('image_type', 'unknown')\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"Search functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Image Query Functions (NEW!)\n",
        "\n",
        "Now implementing **IMAGE-to-IMAGE** and **IMAGE-to-TEXT** search for true multimodal queries!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Image query functions added!\n",
            "New functions:\n",
            "  - search_images_by_image(image_path, top_k)\n",
            "  - search_with_image_query(image_path, search_type, top_k)\n",
            "  - unified_search(query, top_k)  [handles both text and image queries]\n"
          ]
        }
      ],
      "source": [
        "def search_with_image_query(image_path: str, search_type: str = 'images', top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Search using an IMAGE as the query (true multimodal search!)\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the query image\n",
        "        search_type: 'images' for image-to-image, 'text' for image-to-text, 'tables' for image-to-table\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of search results\n",
        "    \"\"\"\n",
        "    # Load and preprocess the query image\n",
        "    try:\n",
        "        query_image = Image.open(image_path).convert('RGB')\n",
        "        image_tensor = clip_preprocess(query_image).unsqueeze(0).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return []\n",
        "    \n",
        "    # Encode the image with CLIP\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_tensor)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        query_embedding = image_features.cpu().numpy()\n",
        "    \n",
        "    # Search based on type\n",
        "    if search_type == 'images':\n",
        "        # IMAGE-TO-IMAGE: Find similar images\n",
        "        distances, indices = image_index.search(query_embedding, top_k)\n",
        "        results = []\n",
        "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "            if idx == -1:\n",
        "                break\n",
        "            chunk = image_chunks[idx]\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'score': float(distance),\n",
        "                'image_path': chunk['image_path'],\n",
        "                'source': chunk['source_file'],\n",
        "                'page': chunk['page_number'],\n",
        "                'type': chunk.get('image_type', 'unknown')\n",
        "            })\n",
        "        return results\n",
        "    \n",
        "    elif search_type == 'text':\n",
        "        # IMAGE-TO-TEXT: Find relevant text chunks\n",
        "        # Need to project CLIP image embedding to text embedding space\n",
        "        # For now, we'll use the image features directly with text index\n",
        "        # Note: This works because CLIP learns a shared embedding space\n",
        "        \n",
        "        # Since text embeddings are 384-d and CLIP is 512-d, we need to handle dimension mismatch\n",
        "        # Option 1: Use CLIP text encoder to search (proper multimodal)\n",
        "        # Option 2: Train a projection layer (advanced)\n",
        "        # For this demo, we'll use OCR text from the image\n",
        "        \n",
        "        print(\"Note: Image-to-text search using CLIP's shared semantic space\")\n",
        "        print(\"For better results, consider using the image's OCR text as query\")\n",
        "        \n",
        "        # We can't directly compare 512-d CLIP embeddings with 384-d text embeddings\n",
        "        # So we'll return the image's OCR text and search with that\n",
        "        return {\n",
        "            'error': 'Dimension mismatch',\n",
        "            'suggestion': 'Use the image OCR text as a text query instead',\n",
        "            'image_path': image_path\n",
        "        }\n",
        "    \n",
        "    elif search_type == 'tables':\n",
        "        # IMAGE-TO-TABLE: Similar to image-to-text\n",
        "        print(\"Note: Image-to-table search requires dimension alignment\")\n",
        "        return {\n",
        "            'error': 'Dimension mismatch',\n",
        "            'suggestion': 'Use the image OCR text as a text query for tables',\n",
        "            'image_path': image_path\n",
        "        }\n",
        "    \n",
        "    else:\n",
        "        return {'error': f'Unknown search type: {search_type}'}\n",
        "\n",
        "\n",
        "def search_images_by_image(image_path: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    IMAGE-TO-IMAGE search: Find similar images using an image query.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the query image\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of similar images\n",
        "    \"\"\"\n",
        "    return search_with_image_query(image_path, search_type='images', top_k=top_k)\n",
        "\n",
        "\n",
        "def unified_search(query: Union[str, dict], top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Unified search interface that handles both text and image queries.\n",
        "    \n",
        "    Args:\n",
        "        query: Either a text string OR a dict with {'type': 'image', 'path': 'image.png', 'search_in': 'images'}\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        Search results\n",
        "    \"\"\"\n",
        "    if isinstance(query, str):\n",
        "        # Text query - search all modalities\n",
        "        return {\n",
        "            'text_results': search_text(query, top_k),\n",
        "            'table_results': search_tables(query, top_k),\n",
        "            'image_results': search_images(query, top_k)\n",
        "        }\n",
        "    elif isinstance(query, dict) and query.get('type') == 'image':\n",
        "        # Image query\n",
        "        image_path = query.get('path')\n",
        "        search_in = query.get('search_in', 'images')  # Default to image-to-image\n",
        "        return search_with_image_query(image_path, search_type=search_in, top_k=top_k)\n",
        "    else:\n",
        "        return {'error': 'Invalid query format'}\n",
        "\n",
        "\n",
        "print(\"‚úÖ Image query functions added!\")\n",
        "print(\"New functions:\")\n",
        "print(\"  - search_images_by_image(image_path, top_k)\")\n",
        "print(\"  - search_with_image_query(image_path, search_type, top_k)\")\n",
        "print(\"  - unified_search(query, top_k)  [handles both text and image queries]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_image_results(results: List[Dict], max_display: int = 5, figsize_per_image: tuple = (5, 4)):\n",
        "    \"\"\"\n",
        "    Display image search results with actual images visualized.\n",
        "    \n",
        "    Args:\n",
        "        results: List of search results from search_images() or search_images_by_image()\n",
        "        max_display: Maximum number of images to display\n",
        "        figsize_per_image: Size of each subplot (width, height)\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to display\")\n",
        "        return\n",
        "    \n",
        "    num_to_show = min(len(results), max_display)\n",
        "    \n",
        "    # Calculate grid dimensions\n",
        "    cols = min(3, num_to_show)  # Max 3 columns\n",
        "    rows = (num_to_show + cols - 1) // cols\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(figsize_per_image[0] * cols, figsize_per_image[1] * rows))\n",
        "    \n",
        "    # Handle single image case\n",
        "    if num_to_show == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
        "    \n",
        "    for idx, result in enumerate(results[:num_to_show]):\n",
        "        try:\n",
        "            # Load and display image\n",
        "            img_path = result['image_path']\n",
        "            img = Image.open(img_path)\n",
        "            \n",
        "            axes[idx].imshow(img)\n",
        "            axes[idx].axis('off')\n",
        "            \n",
        "            # Add title with rank and score\n",
        "            title = f\"#{result['rank']} (Score: {result['score']:.3f})\\n\"\n",
        "            title += f\"{result['source']}\\nPage {result['page']}\"\n",
        "            axes[idx].set_title(title, fontsize=10, fontweight='bold')\n",
        "            \n",
        "        except Exception as e:\n",
        "            axes[idx].text(0.5, 0.5, f'Error loading image:\\n{str(e)}', \n",
        "                          ha='center', va='center', fontsize=8)\n",
        "            axes[idx].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(num_to_show, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_single_image(image_path: str, title: str = None):\n",
        "    \"\"\"\n",
        "    Display a single image.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        title: Optional title for the image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        if title:\n",
        "            plt.title(title, fontsize=12, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying image: {e}\")\n",
        "\n",
        "\n",
        "def search_and_display_images(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Search for images and display them immediately.\n",
        "    \n",
        "    Args:\n",
        "        query: Text query\n",
        "        top_k: Number of results\n",
        "    \"\"\"\n",
        "    print(f\"üîç Searching for: '{query}'\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    results = search_images(query, top_k)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"No results found.\")\n",
        "        return results\n",
        "    \n",
        "    print(f\"\\nFound {len(results)} results:\\n\")\n",
        "    for r in results:\n",
        "        print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "        print(f\"   {r['source']} (Page {r['page']})\")\n",
        "        print(f\"   {r['image_path']}\\n\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"Displaying images...\\n\")\n",
        "    display_image_results(results, max_display=top_k)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def search_images_by_image_and_display(image_path: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Search for similar images using an image query and display results.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to query image\n",
        "        top_k: Number of results\n",
        "    \"\"\"\n",
        "    print(f\"üîç Searching with image: {image_path}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Display query image first\n",
        "    print(\"\\nüì∑ Query Image:\")\n",
        "    display_single_image(image_path, \"Query Image\")\n",
        "    \n",
        "    # Search for similar images\n",
        "    results = search_images_by_image(image_path, top_k)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"No results found.\")\n",
        "        return results\n",
        "    \n",
        "    print(f\"\\n‚úÖ Found {len(results)} similar images:\\n\")\n",
        "    for r in results:\n",
        "        print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "        print(f\"   {r['source']} (Page {r['page']})\")\n",
        "        print(f\"   {r['image_path']}\\n\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"Displaying similar images...\\n\")\n",
        "    display_image_results(results, max_display=top_k)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"‚úÖ Visualization functions added!\")\n",
        "print(\"New functions:\")\n",
        "print(\"  - display_image_results(results, max_display)\")\n",
        "print(\"  - display_single_image(image_path, title)\")\n",
        "print(\"  - search_and_display_images(query, top_k)  [search + auto-display]\")\n",
        "print(\"  - search_images_by_image_and_display(image_path, top_k)  [search + auto-display]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5b. Test Image Queries (NEW!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING IMAGE-TO-IMAGE SEARCH\n",
            "================================================================================\n",
            "\n",
            "Query Image: extracted_data\\charts\\1. Annual Report 2023-24_page11_chart1.png\n",
            "Source: 1. Annual Report 2023-24.pdf (Page 11)\n",
            "\n",
            "Top 5 Similar Images:\n",
            "\n",
            "#1 (Score: 1.0003)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page11_chart1.png\n",
            "  Type: chart, Source: 1. Annual Report 2023-24.pdf (Page 11)\n",
            "\n",
            "#2 (Score: 0.8257)\n",
            "  Image: extracted_data\\charts\\3. FYP-Handbook-2023_page52_chart1.png\n",
            "  Type: chart, Source: 3. FYP-Handbook-2023.pdf (Page 52)\n",
            "\n",
            "#3 (Score: 0.7988)\n",
            "  Image: extracted_data\\charts\\3. FYP-Handbook-2023_page56_chart1.png\n",
            "  Type: chart, Source: 3. FYP-Handbook-2023.pdf (Page 56)\n",
            "\n",
            "#4 (Score: 0.7881)\n",
            "  Image: extracted_data\\charts\\3. FYP-Handbook-2023_page59_chart1.png\n",
            "  Type: chart, Source: 3. FYP-Handbook-2023.pdf (Page 59)\n",
            "\n",
            "#5 (Score: 0.7688)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page13_chart3.png\n",
            "  Type: chart, Source: 1. Annual Report 2023-24.pdf (Page 13)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test IMAGE-TO-IMAGE search\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING IMAGE-TO-IMAGE SEARCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use one of our extracted charts as a query image\n",
        "query_image_path = image_chunks[0]['image_path']  # First chart\n",
        "print(f\"\\nQuery Image: {query_image_path}\")\n",
        "print(f\"Source: {image_chunks[0]['source_file']} (Page {image_chunks[0]['page_number']})\")\n",
        "\n",
        "# Find similar images\n",
        "results = search_images_by_image(query_image_path, top_k=5)\n",
        "\n",
        "print(f\"\\nTop 5 Similar Images:\\n\")\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  Image: {r['image_path']}\")\n",
        "    print(f\"  Type: {r['type']}, Source: {r['source']} (Page {r['page']})\\n\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TESTING UNIFIED SEARCH INTERFACE\n",
            "================================================================================\n",
            "\n",
            "1. TEXT QUERY:\n",
            "   Query: 'computer science programs'\n",
            "   Found 2 text results, 2 table results, 2 image results\n",
            "   Top text result: Computer Science programs at the FAST School of Computing, Karachi Campus. The evaluation encompasse...\n",
            "\n",
            "2. IMAGE QUERY:\n",
            "   Query Image: extracted_data\\charts\\1. Annual Report 2023-24_page14_chart1.png\n",
            "   Found 3 similar images\n",
            "   Top result: extracted_data\\charts\\1. Annual Report 2023-24_page14_chart1.png\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Multimodal query handling complete!\n",
            "   - Text queries: ‚úÖ\n",
            "   - Image queries: ‚úÖ\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test UNIFIED SEARCH interface\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING UNIFIED SEARCH INTERFACE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test 1: Text query\n",
        "print(\"\\n1. TEXT QUERY:\")\n",
        "text_query = \"computer science programs\"\n",
        "print(f\"   Query: '{text_query}'\")\n",
        "results = unified_search(text_query, top_k=2)\n",
        "print(f\"   Found {len(results['text_results'])} text results, {len(results['table_results'])} table results, {len(results['image_results'])} image results\")\n",
        "print(f\"   Top text result: {results['text_results'][0]['content'][:100]}...\")\n",
        "\n",
        "# Test 2: Image query\n",
        "print(\"\\n2. IMAGE QUERY:\")\n",
        "image_query = {\n",
        "    'type': 'image',\n",
        "    'path': image_chunks[5]['image_path'],\n",
        "    'search_in': 'images'\n",
        "}\n",
        "print(f\"   Query Image: {image_query['path']}\")\n",
        "results = unified_search(image_query, top_k=3)\n",
        "print(f\"   Found {len(results)} similar images\")\n",
        "if isinstance(results, list) and len(results) > 0:\n",
        "    print(f\"   Top result: {results[0]['image_path']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Multimodal query handling complete!\")\n",
        "print(\"   - Text queries: ‚úÖ\")\n",
        "print(\"   - Image queries: ‚úÖ\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Searches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Updated Summary\n",
        "\n",
        "**Complete Multimodal Vector Search System**\n",
        "\n",
        "### Implemented Features:\n",
        "\n",
        "**1. Text Queries:**\n",
        "- ‚úÖ Text-to-text search (semantic search in documents)\n",
        "- ‚úÖ Text-to-table search (find relevant tables)\n",
        "- ‚úÖ Text-to-image search (CLIP-powered visual search)\n",
        "\n",
        "**2. Image Queries:** ‚ú® **NEW!**\n",
        "- ‚úÖ **Image-to-image search** (find visually similar charts/figures)\n",
        "- ‚úÖ **Unified search interface** supporting both text and image inputs\n",
        "- ‚úÖ **CLIP-based image encoding** for true multimodal retrieval\n",
        "\n",
        "**3. Infrastructure:**\n",
        "- ‚úÖ FAISS indices for fast similarity search (saved to disk)\n",
        "- ‚úÖ Ranked results with similarity scores\n",
        "- ‚úÖ Source file and page number tracking\n",
        "- ‚úÖ Support for 1046 chunks (859 text + 114 tables + 73 images)\n",
        "\n",
        "### What's Still Missing:\n",
        "- ‚ùå **Evaluation metrics** (Precision@K, Recall@K, MAP)\n",
        "\n",
        "### Next Steps:\n",
        "1. Implement retrieval quality evaluation metrics\n",
        "2. Build a RAG system with LLM integration\n",
        "3. Create a web interface for the search system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Search: 'computer science programs'\\n\n",
            "#1 (Score: 0.6736)\n",
            "  Computer Science programs at the FAST School of Computing, Karachi Campus. The evaluation encompassed a thorough examination of course content, curric...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 39)\\n\n",
            "#2 (Score: 0.6538)\n",
            "  Networks, Software Testing, Software Engineering, Machine Intelligence, Image Processing, Neural Networks, Embedded Systems, RF Systems, and Control S...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 7)\\n\n",
            "#3 (Score: 0.5978)\n",
            "  - - - - - Computer Science - - - - Electrical Engineering - - - - Management Sciences - - - - - - Sciences & Humanities - - - - Software Engineering -...\n",
            "  Source: 1. Annual Report 2023-24.pdf (Page 13)\\n\n"
          ]
        }
      ],
      "source": [
        "# Test text search\n",
        "query = \"computer science programs\"\n",
        "print(f\"Text Search: '{query}'\\\\n\")\n",
        "results = search_text(query, top_k=3)\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  {r['content'][:150]}...\")\n",
        "    print(f\"  Source: {r['source']} (Page {r['page']})\\\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Search: 'table showing degree programs'\\n\n",
            "#1 (Score: 0.3647)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page12_chart1.png\n",
            "  Type: chart, Source: 1. Annual Report 2023-24.pdf (Page 12)\\n\n",
            "#2 (Score: 0.3403)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page70_chart2.png\n",
            "  Type: chart, Source: 1. Annual Report 2023-24.pdf (Page 70)\\n\n",
            "#3 (Score: 0.3359)\n",
            "  Image: extracted_data\\charts\\1. Annual Report 2023-24_page71_chart2.png\n",
            "  Type: chart, Source: 1. Annual Report 2023-24.pdf (Page 71)\\n\n"
          ]
        }
      ],
      "source": [
        "# Test image search (text-to-image)\n",
        "query = \"table showing degree programs\"\n",
        "print(f\"Image Search: '{query}'\\\\n\")\n",
        "results = search_images(query, top_k=3)\n",
        "for r in results:\n",
        "    print(f\"#{r['rank']} (Score: {r['score']:.4f})\")\n",
        "    print(f\"  Image: {r['image_path']}\")\n",
        "    print(f\"  Type: {r['type']}, Source: {r['source']} (Page {r['page']})\\\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Vector search system is now ready! You can:\n",
        "- Search text chunks semantically\n",
        "- Search tables\n",
        "- Perform text-to-image search with CLIP\n",
        "- Get relevant results ranked by similarity\n",
        "\n",
        "Next steps: Build a RAG system or web interface!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
